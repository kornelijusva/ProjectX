{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "<br/> Prof. Dr. Georgios K. Ouzounis\n",
    "<br/> email: georgios.ouzounis@go.kauko.lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Data loading and pre-processing\n",
    "2. Building the RNN\n",
    "3. Train and deploy the RNN\n",
    "4. Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.sapientrade.com/images/2017/02/14/AI-Machine-Learning-Trading-Benefits.jpg\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 5-year history of the Google Stock prices predict the stock values for the period of the recent most month that are not included in the historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data-sets\n",
    "\n",
    "The data-sets are two comma-separated values files (CSV) and contain a data table of 1258 records for training and a table of 21 records for testing.\n",
    "\n",
    "They can be found at the [Kaggle.com website](https://www.kaggle.com/akram24/google-stock-price-train) or at various web locations after searching for the filenames: \n",
    "\n",
    "**Google_Stock_Price_Test.csv** and **Google_Stock_Price_Train.csv**\n",
    "\n",
    "Known alternative location: Github user [pdway53](https://github.com/pdway53/Predict_Google_Stock_Price_RNN) \n",
    "\n",
    "Open a terminal and use the wget command to get it of the selected location. Example:\n",
    "\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/pdway53/Predict_Google_Stock_Price_RNN/master/Google_Stock_Price_Test.csv\n",
    "\n",
    "wget https://raw.githubusercontent.com/pdway53/Predict_Google_Stock_Price_RNN/master/Google_Stock_Price_Train.csv \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "We need 3 main libraries:\n",
    "\n",
    "- [Numpy](http://www.numpy.org): it is the fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object that can be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined.\n",
    "- [matplotlib](https://matplotlib.org):  it is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "- [pandas](https://pandas.pydata.org): is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description: the open high, low and close values of the Google Stock from 2012 to 2016. [Relevant code here](https://github.com/pdway53/Predict_Google_Stock_Price_RNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "\n",
    "# load the file contents \n",
    "dataset_train = pd.read_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-07-01</td>\n",
       "      <td>41.150002</td>\n",
       "      <td>43.810001</td>\n",
       "      <td>41.009998</td>\n",
       "      <td>42.520000</td>\n",
       "      <td>42.520000</td>\n",
       "      <td>5933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-07-02</td>\n",
       "      <td>41.330002</td>\n",
       "      <td>42.160000</td>\n",
       "      <td>39.060001</td>\n",
       "      <td>39.689999</td>\n",
       "      <td>39.689999</td>\n",
       "      <td>2730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-07-06</td>\n",
       "      <td>39.709999</td>\n",
       "      <td>39.709999</td>\n",
       "      <td>36.919998</td>\n",
       "      <td>37.299999</td>\n",
       "      <td>37.299999</td>\n",
       "      <td>2627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-07-07</td>\n",
       "      <td>37.259998</td>\n",
       "      <td>38.740002</td>\n",
       "      <td>36.419998</td>\n",
       "      <td>38.299999</td>\n",
       "      <td>38.299999</td>\n",
       "      <td>2153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-07-08</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.220001</td>\n",
       "      <td>36.889999</td>\n",
       "      <td>38.209999</td>\n",
       "      <td>38.209999</td>\n",
       "      <td>2341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>2019-07-24</td>\n",
       "      <td>31.280001</td>\n",
       "      <td>32.380001</td>\n",
       "      <td>30.730000</td>\n",
       "      <td>32.259998</td>\n",
       "      <td>32.259998</td>\n",
       "      <td>462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>2019-07-25</td>\n",
       "      <td>32.160000</td>\n",
       "      <td>32.180000</td>\n",
       "      <td>31.490000</td>\n",
       "      <td>31.719999</td>\n",
       "      <td>31.719999</td>\n",
       "      <td>310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2019-07-26</td>\n",
       "      <td>31.780001</td>\n",
       "      <td>32.209999</td>\n",
       "      <td>31.350000</td>\n",
       "      <td>32.040001</td>\n",
       "      <td>32.040001</td>\n",
       "      <td>260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>2019-07-29</td>\n",
       "      <td>31.959999</td>\n",
       "      <td>32.020000</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>31.469999</td>\n",
       "      <td>31.469999</td>\n",
       "      <td>291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>2019-07-30</td>\n",
       "      <td>30.820000</td>\n",
       "      <td>32.380001</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>596400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2537 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2009-07-01  41.150002  43.810001  41.009998  42.520000  42.520000   \n",
       "1     2009-07-02  41.330002  42.160000  39.060001  39.689999  39.689999   \n",
       "2     2009-07-06  39.709999  39.709999  36.919998  37.299999  37.299999   \n",
       "3     2009-07-07  37.259998  38.740002  36.419998  38.299999  38.299999   \n",
       "4     2009-07-08  38.000000  38.220001  36.889999  38.209999  38.209999   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "2532  2019-07-24  31.280001  32.380001  30.730000  32.259998  32.259998   \n",
       "2533  2019-07-25  32.160000  32.180000  31.490000  31.719999  31.719999   \n",
       "2534  2019-07-26  31.780001  32.209999  31.350000  32.040001  32.040001   \n",
       "2535  2019-07-29  31.959999  32.020000  31.250000  31.469999  31.469999   \n",
       "2536  2019-07-30  30.820000  32.380001  30.500000  32.000000  32.000000   \n",
       "\n",
       "       Volume  \n",
       "0     5933800  \n",
       "1     2730900  \n",
       "2     2627800  \n",
       "3     2153200  \n",
       "4     2341000  \n",
       "...       ...  \n",
       "2532   462800  \n",
       "2533   310900  \n",
       "2534   260700  \n",
       "2535   291000  \n",
       "2536   596400  \n",
       "\n",
       "[2537 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subtable of relevant entries (open values)\n",
    "# The .values makes this vector a numpy array\n",
    "training_set = dataset_train.iloc[:, 1:2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41.150002],\n",
       "       [41.330002],\n",
       "       [39.709999],\n",
       "       ...,\n",
       "       [31.780001],\n",
       "       [31.959999],\n",
       "       [30.82    ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays do not support the view() and head() methods. [More on accessing the numpy data](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Next we need to rescale our data to the range from 0 to 1. \n",
    "\n",
    "Feature scaling is essential as discussed if the Features lecture and needs to be applied to both the training and test sets.\n",
    "\n",
    "It is computed using the ScikitLearn library [MinMaxScaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) which transforms the selected feature by scaling it to a given range. If more than one, this estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# import the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler instance to rescale all data to the range of 0.0 to 1.0 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54566605],\n",
       "       [0.54845718],\n",
       "       [0.52333696],\n",
       "       ...,\n",
       "       [0.40037218],\n",
       "       [0.40316328],\n",
       "       [0.38548614]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 90 timesteps and 1 output\n",
    "\n",
    "# the 90 stock prices in the last 3 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2537, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stock price today\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start from day 90 because that is the first instance allowing us to \n",
    "# go back 90 days\n",
    "for i in range(90, 1258): \n",
    "    # 0 is the column ID, the only column in this case.    \n",
    "    # put the last 60 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-90:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54566605, 0.54845718, 0.52333696, ..., 0.42208096, 0.46162198,\n",
       "        0.44208408],\n",
       "       [0.54845718, 0.52333696, 0.48534656, ..., 0.46162198, 0.44208408,\n",
       "        0.44394482],\n",
       "       [0.52333696, 0.48534656, 0.49682124, ..., 0.44208408, 0.44394482,\n",
       "        0.45247326],\n",
       "       ...,\n",
       "       [0.46332767, 0.47123584, 0.46503338, ..., 0.49061869, 0.49852694,\n",
       "        0.49759653],\n",
       "       [0.47123584, 0.46503338, 0.48131495, ..., 0.49852694, 0.49759653,\n",
       "        0.49992251],\n",
       "       [0.46503338, 0.48131495, 0.47542255, ..., 0.49759653, 0.49992251,\n",
       "        0.48146999]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44394482, 0.45247326, 0.45697009, ..., 0.49992251, 0.48146999,\n",
       "       0.48643206])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Matrix\n",
    "\n",
    "We need to add a new matrix dimension to accommodate the indicator (predictor). \n",
    "\n",
    "NumPy matrices are tensors (3D) and essentially we need to specify that our matrix consists of **60 days** (dimension x) times **total days in data set** (dimension y) times **1 value per matrix cell (scalar)** (dimension z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to add the stock value of somebody else together with the the past 60 days of Google, we need to change the length of the 3 dimension to  2.  RNN training tables are 3D!!! Read: [Reshaping NumPy Array | Numpy Array Reshape Examples](https://backtobazics.com/python/python-reshaping-numpy-array-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data matrix, we retain the 2 original dimensions and add a third of depth=1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.54566605],\n",
       "        [0.54845718],\n",
       "        [0.52333696],\n",
       "        ...,\n",
       "        [0.42208096],\n",
       "        [0.46162198],\n",
       "        [0.44208408]],\n",
       "\n",
       "       [[0.54845718],\n",
       "        [0.52333696],\n",
       "        [0.48534656],\n",
       "        ...,\n",
       "        [0.46162198],\n",
       "        [0.44208408],\n",
       "        [0.44394482]],\n",
       "\n",
       "       [[0.52333696],\n",
       "        [0.48534656],\n",
       "        [0.49682124],\n",
       "        ...,\n",
       "        [0.44208408],\n",
       "        [0.44394482],\n",
       "        [0.45247326]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.46332767],\n",
       "        [0.47123584],\n",
       "        [0.46503338],\n",
       "        ...,\n",
       "        [0.49061869],\n",
       "        [0.49852694],\n",
       "        [0.49759653]],\n",
       "\n",
       "       [[0.47123584],\n",
       "        [0.46503338],\n",
       "        [0.48131495],\n",
       "        ...,\n",
       "        [0.49852694],\n",
       "        [0.49759653],\n",
       "        [0.49992251]],\n",
       "\n",
       "       [[0.46503338],\n",
       "        [0.48131495],\n",
       "        [0.47542255],\n",
       "        ...,\n",
       "        [0.49759653],\n",
       "        [0.49992251],\n",
       "        [0.48146999]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN initialization\n",
    "\n",
    "- Import the sequential model from the Keras API;\n",
    "- Import the Dense layer template from the Keras API;\n",
    "- Import the LSTM model from the Keras API\n",
    "- Create an instance of the sequential model called regressor because we want to predict a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as a sequence of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add First Layer\n",
    "\n",
    "We first add an object of the LSTM class! \n",
    "\n",
    "- The first argument is the number of units or LSTM memory cells. Include many neurons to address the high dimensionality of the problem; say 50 neurons! \n",
    "- Second arg: return sequences = true; stacked LSTM !\n",
    "- Third arg: input 3D shape: observations vs time steps vs number of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the argument is the dropout rate to ignore in the layers (20%), \n",
    "# i.e. 50 units * 20% = 10 units will be dropped each time\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers\n",
    "\n",
    "We can add more LSTM layers but along with Dropout regularization to make sure we avoid overfitting! \n",
    "\n",
    "We don’t need to add the shape of the layer again because it is recognized automatically from the number of input units.\n",
    "\n",
    "The last layer does not return a sequence but connected directly to a fully connected output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# we removed the return_sequences because we no longer return a \n",
    "# sequence but a value instead\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer & Compile\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop is recommended in the Keras documentation. The Adam optimizer is also a powerful choice.\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the Training set\n",
    "\n",
    "We now want to train our RNN using the data in our **Training Set X** and **predictors in y** (ground truth in this case). Parameters that can be specified are the:\n",
    "\n",
    "- **Batch size**:  update the cell weights not on every stock price on every batch_size values; \n",
    "- **Number of epochs**: how many iterations to be used, i.e. number of forward and backward propagations for the update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.0184\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 7s 193ms/step - loss: 0.0075\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0066\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0066\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 7s 178ms/step - loss: 0.0064\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0057\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0054\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 7s 177ms/step - loss: 0.0050\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0047\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 0.0044\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.0043\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0049\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0042\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.0041\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0048\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0041\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 0.0035\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 7s 176ms/step - loss: 0.0039\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.0035\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0034\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0032\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0031\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0032\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0032\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 6s 169ms/step - loss: 0.0030\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 6s 171ms/step - loss: 0.0036\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 6s 173ms/step - loss: 0.0032\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0029\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 6s 170ms/step - loss: 0.0027\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 7s 177ms/step - loss: 0.0029\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0030\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0027\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0026\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0026\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0030\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0026\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0026\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0024\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0025\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0023\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0024\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0024\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0027\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0023\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0026\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0023\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0021\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0022\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0022\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 6s 169ms/step - loss: 0.0020\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0020\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0021\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0019\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 6s 167ms/step - loss: 0.0020\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0021\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0021\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0019\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0020\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0018\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 6s 167ms/step - loss: 0.0020\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0019\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 6s 169ms/step - loss: 0.0018\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0017\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0018\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 6s 172ms/step - loss: 0.0018\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0017\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0016\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0019\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0016\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0016\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 6s 158ms/step - loss: 0.0015\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0016\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 6s 158ms/step - loss: 0.0016\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0014\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 6s 169ms/step - loss: 0.0015\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 6s 167ms/step - loss: 0.0014\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 6s 171ms/step - loss: 0.0016\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0013\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0014\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 6s 172ms/step - loss: 0.0015\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0016\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0018\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 7s 177ms/step - loss: 0.0016\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 6s 172ms/step - loss: 0.0014\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 7s 176ms/step - loss: 0.0014\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 6s 172ms/step - loss: 0.0013\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 6s 170ms/step - loss: 0.0013\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0012\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0013\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 6s 172ms/step - loss: 0.0013\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0012\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0013\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0013\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0011\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 0.0011\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 0.0012\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 0.0011\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82901e32b0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Predictions\n",
    "\n",
    "Create a data-frame by importing the Google Stock Price Test set for January 2017 using pandas and make it a numpy array.\n",
    "\n",
    "There are 20 financial days in one month, weekends are excluded!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>31.209999</td>\n",
       "      <td>31.209999</td>\n",
       "      <td>29.850000</td>\n",
       "      <td>30.350000</td>\n",
       "      <td>30.350000</td>\n",
       "      <td>462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>30.080000</td>\n",
       "      <td>30.370001</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>29.030001</td>\n",
       "      <td>29.030001</td>\n",
       "      <td>573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>28.010000</td>\n",
       "      <td>28.280001</td>\n",
       "      <td>27.040001</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>27.700001</td>\n",
       "      <td>28.059999</td>\n",
       "      <td>26.590000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>564300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>26.549999</td>\n",
       "      <td>26.760000</td>\n",
       "      <td>25.610001</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>26.180000</td>\n",
       "      <td>26.860001</td>\n",
       "      <td>26.180000</td>\n",
       "      <td>26.299999</td>\n",
       "      <td>26.299999</td>\n",
       "      <td>370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>26.160000</td>\n",
       "      <td>26.160000</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>25.260000</td>\n",
       "      <td>25.260000</td>\n",
       "      <td>579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>24.160000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>24.469999</td>\n",
       "      <td>25.980000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>25.490000</td>\n",
       "      <td>25.490000</td>\n",
       "      <td>557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>24.799999</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>23.700001</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>24.020000</td>\n",
       "      <td>24.020000</td>\n",
       "      <td>22.459999</td>\n",
       "      <td>22.809999</td>\n",
       "      <td>22.809999</td>\n",
       "      <td>371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-08-16</td>\n",
       "      <td>23.040001</td>\n",
       "      <td>24.049999</td>\n",
       "      <td>23.040001</td>\n",
       "      <td>23.940001</td>\n",
       "      <td>23.940001</td>\n",
       "      <td>338800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>24.530001</td>\n",
       "      <td>24.740000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.020000</td>\n",
       "      <td>24.020000</td>\n",
       "      <td>291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>23.930000</td>\n",
       "      <td>23.930000</td>\n",
       "      <td>23.020000</td>\n",
       "      <td>23.200001</td>\n",
       "      <td>23.200001</td>\n",
       "      <td>236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-08-21</td>\n",
       "      <td>23.650000</td>\n",
       "      <td>24.260000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>24.280001</td>\n",
       "      <td>24.480000</td>\n",
       "      <td>23.719999</td>\n",
       "      <td>23.879999</td>\n",
       "      <td>23.879999</td>\n",
       "      <td>331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-08-23</td>\n",
       "      <td>23.549999</td>\n",
       "      <td>23.549999</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.360001</td>\n",
       "      <td>22.360001</td>\n",
       "      <td>394300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>22.760000</td>\n",
       "      <td>22.780001</td>\n",
       "      <td>22.240000</td>\n",
       "      <td>22.480000</td>\n",
       "      <td>22.480000</td>\n",
       "      <td>259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>22.620001</td>\n",
       "      <td>22.670000</td>\n",
       "      <td>21.350000</td>\n",
       "      <td>21.629999</td>\n",
       "      <td>21.629999</td>\n",
       "      <td>496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>21.510000</td>\n",
       "      <td>22.040001</td>\n",
       "      <td>21.320000</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>22.040001</td>\n",
       "      <td>23.120001</td>\n",
       "      <td>22.040001</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>23.080000</td>\n",
       "      <td>23.379999</td>\n",
       "      <td>22.469999</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>392400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  Volume\n",
       "0   2019-08-01  31.209999  31.209999  29.850000  30.350000  30.350000  462800\n",
       "1   2019-08-02  30.080000  30.370001  28.600000  29.030001  29.030001  573600\n",
       "2   2019-08-05  28.010000  28.280001  27.040001  27.400000  27.400000  526900\n",
       "3   2019-08-06  27.700001  28.059999  26.590000  27.000000  27.000000  564300\n",
       "4   2019-08-07  26.549999  26.760000  25.610001  25.900000  25.900000  522400\n",
       "5   2019-08-08  26.180000  26.860001  26.180000  26.299999  26.299999  370000\n",
       "6   2019-08-09  26.160000  26.160000  25.080000  25.260000  25.260000  579500\n",
       "7   2019-08-12  25.010000  25.010000  24.160000  24.500000  24.500000  350200\n",
       "8   2019-08-13  24.469999  25.980000  24.250000  25.490000  25.490000  557800\n",
       "9   2019-08-14  24.799999  24.990000  23.700001  23.900000  23.900000  435500\n",
       "10  2019-08-15  24.020000  24.020000  22.459999  22.809999  22.809999  371100\n",
       "11  2019-08-16  23.040001  24.049999  23.040001  23.940001  23.940001  338800\n",
       "12  2019-08-19  24.530001  24.740000  24.000000  24.020000  24.020000  291400\n",
       "13  2019-08-20  23.930000  23.930000  23.020000  23.200001  23.200001  236500\n",
       "14  2019-08-21  23.650000  24.260000  23.250000  24.100000  24.100000  352100\n",
       "15  2019-08-22  24.280001  24.480000  23.719999  23.879999  23.879999  331000\n",
       "16  2019-08-23  23.549999  23.549999  22.250000  22.360001  22.360001  394300\n",
       "17  2019-08-26  22.760000  22.780001  22.240000  22.480000  22.480000  259000\n",
       "18  2019-08-27  22.620001  22.670000  21.350000  21.629999  21.629999  496600\n",
       "19  2019-08-28  21.510000  22.040001  21.320000  21.700001  21.700001  328200\n",
       "20  2019-08-29  22.040001  23.120001  22.040001  22.940001  22.940001  365900\n",
       "21  2019-08-30  23.080000  23.379999  22.469999  23.000000  23.000000  392400"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the real stock price for February 1st 2012 - \n",
    "# January 31st 2017\n",
    "\n",
    "dataset_test = pd.read_csv('test_data.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31.209999],\n",
       "       [30.08    ],\n",
       "       [28.01    ],\n",
       "       [27.700001],\n",
       "       [26.549999],\n",
       "       [26.18    ],\n",
       "       [26.16    ],\n",
       "       [25.01    ],\n",
       "       [24.469999],\n",
       "       [24.799999],\n",
       "       [24.02    ],\n",
       "       [23.040001],\n",
       "       [24.530001],\n",
       "       [23.93    ],\n",
       "       [23.65    ],\n",
       "       [24.280001],\n",
       "       [23.549999],\n",
       "       [22.76    ],\n",
       "       [22.620001],\n",
       "       [21.51    ],\n",
       "       [22.040001],\n",
       "       [23.08    ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the stock price value for each day in January 2017, we need the values in the last 60 days.\n",
    "\n",
    "To obtain this **history** we need to combine both the training and test sets in one.\n",
    "\n",
    "If we were to use the training_set and test_set we would need to use the scaler  but that would change the actual test values.  Thus concatenate the original data frames!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "\n",
    "# axis = 0 means concatenate the lines (i.e. vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2559"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the difference in the length of the first two gives us \n",
    "# the first day in 2017, and we need to go back 90 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did not use iloc from panda so lets reshape the numpy array for \n",
    "# compatibility: i.e. all the values from input lines to be stacked in one \n",
    "# column. The -1 means that the numpy has no knowledge of how the \n",
    "# values were stored in lines. The 1 means we want to them in one \n",
    "# column.\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# apply the feature scaler\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each price in Jan. 2017 we need the **immediate 60 values** before it. \n",
    "2. We have 21 prices in January;\n",
    "3. We need a numpy 3D array of 60 prices (columns) times 21 days (rows) times 1 dependent variable \n",
    "4. We don’t need y_test. That is what we are trying to compute!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 60 from inputs are from training set; start \n",
    "# from 60 and get the extra 20, i.e. up to 80\n",
    "for i in range(90, 112): \n",
    "    X_test.append(inputs[i-90:i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test) # not 3D structure yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to inverse the scaling to get meaningful predicted stock price # outputs\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Navistar International Corporation Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Navistar International Corporation Stock Price')\n",
    "\n",
    "plt.title('Navistar International Corporation Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Navistar International Corporation Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line shows the trend of the stock for the month of January 2017. \n",
    "\n",
    "Some observations:\n",
    "- The prediction lags behind the actual price curve because the model cannot react to fast non-linear changes. Spikes are examples of fast non-linear changes\n",
    "- Model reacts pretty well to smooth changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the RMSE\n",
    "\n",
    "If we need to compute the RMSE for our Stock Price Prediction problem, we use the real stock price and predicted stock price as shown.\n",
    "\n",
    "Then consider dividing this RMSE by the range of the Google Stock Price values of January 2017 to get a relative error, as opposed to an absolute error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.112159882226059"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:22,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data need to be placed in the same order/format  as in the case of the training/test sets.\n",
    "\n",
    "1. Getting more training data: we trained our model on the past 5 years of the  Google Stock Price but it would be even better to train it on the past 10 years.\n",
    "\n",
    "2. Increasing the number of time steps: the model remembered the stock price from the 60 previous financial days to predict the stock price of the next day. That’s because we chose a number of 60 time steps (3 months). You could try to increase the number of time steps, by choosing for example 120 time steps (6 months).\n",
    "\n",
    "3. Adding some other indicators: if you have the financial instinct that the stock price of some other companies might be correlated to the one of Google, you could add this other stock price as a new indicator in the training data.\n",
    "\n",
    "4. Adding more LSTM layers: we built a RNN with four LSTM layers but you could try with even more.\n",
    "\n",
    "5. Adding more neurons in the LSTM layers: we highlighted the fact that we needed a high number of neurons in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurons in each of our 4 LSTM layers. You could try an architecture with even more neurons in each of the 4 (or more) LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the RNN\n",
    "\n",
    "Parameter Tuning on the RNN model: we are dealing with a Regression problem because we predict a continuous outcome.\n",
    "\n",
    "**Tip**: replace: scoring = 'accuracy' by scoring = 'neg_mean_squared_error' in the GridSearchCV class parameters as we did in the ANN case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
